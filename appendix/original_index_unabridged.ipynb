{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='center'>\n",
    "<img src='images/seattlehouses.jpg' style='width:800px;'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doug and the Data Diggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Jakub Rybicki, Chris O'Malley, Doug Mill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to build an inferential linear regression model. Our model will help our stakeholder understand King County home valuations better. We will follow the assumptions of linear regression which are linearity, independence, normality, and homoscedasticity. We will also strive to have a high R^2 value, signaling that our parameters are explaining much of the total variance in house sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our stakeholder is an out-of-town real estate agency interested in opening an office in King County. The county seat is Seattle.\n",
    "\n",
    "We are acting as a consultant for our stakeholder to determine key factors in evaluating a home in King County. We want to provide our client with the math and reasoning behind local home valuations. We will discuss the important features that affect a valuation based on analysis of previous home sales in the area. We will take a look at factors such as home size & space, school district in which the home is located, upgrades & amenities, and local market conditions.\n",
    "\n",
    "Our stakeholder will be able to evaluate the Seattle and King County real estate market by understanding the key variables that affect price from our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original data set includes info about King County homes that sold between May 2nd, 2014 to May 24th, 2015. The target variable is price. We removed properties that were outside of three standard deviations of the mean price. This left us with homes with a price under $1.65M. We also removed typos. That took us from 21597 entries to 21280. We then incorporated 2015 School GIS data from kingcounty.gov in order to create our 'District' variable. The District variable includes the school district that the home is located in. There are 18 school districts in King County. We cleaned the original data set and merged it with the outside data in order to create our cleaned dataset, known as 'KingSchool.csv'. It can be found in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Loading our data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.power import TTestIndPower, TTestPower\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data preparation, we formatted many columns. We later decided we didn't need to use many of them. We ended up eliminating samples where the price was outside of 3 standard deviations of the mean price. We then found a typo and removed it. This is how we cleaned the data from the dataset that was included which was kc_house_data.csv. \n",
    "\n",
    "We incorporated 2015 School GIS data from the kingcounty.gov website. We then merged it with kc_house_data.csv on 4 different keys; 'id', 'lat', 'long', and 'zipcode'. This plotted the entries against district lines, and formed a column in which each entry was classified by the district that it was located in. There are 18 school districts in King County; Mercer Island, Bellevue, Seattle, Lake Washington, Vashon Island, Issaquah, Shoreline, Northshore, Snoqualmie Valley, Riverview, Highline, Renton, Skykomish, Enumclaw, Tahoma, Tukwila, Kent, and Federal Way.\n",
    "\n",
    "Since district contained 18 categories, we changed it into dummy variables. By using get_dummies, District was transformed into 18 seperate variables. Each variable contains a 0 or 1. 0 signals that the house was not located in that district while 1 signals that it is present in that district.\n",
    "\n",
    "We picked which columns that we wanted to focus on originally from a heatmap of correlations. We ran train test splits for every single model that we did. We also made sure to run StandardScaler on every single model we did in order to scale them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/KingSchool.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b68235a189c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/KingSchool.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/KingSchool.csv'"
     ]
    }
   ],
   "source": [
    "df  = pd.read_csv('data/KingSchool.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['District']\n",
    "dummies = pd.get_dummies(df[categoricals], prefix=categoricals, drop_first=True)\n",
    "df = df.drop(categoricals, axis=1)\n",
    "df = pd.concat([df, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['date'],axis=1,inplace=True)\n",
    "df = df.astype(np.float64)\n",
    "df['District_FEDERAL_WAY'] = df['District_FEDERAL WAY']\n",
    "df['District_LAKE_WASHINGTON'] = df['District_LAKE WASHINGTON']\n",
    "df['District_MERCER_ISLAND'] = df['District_MERCER ISLAND']\n",
    "df['District_SNOQUALMIE_VALLEY'] = df['District_SNOQUALMIE VALLEY']\n",
    "df['District_VASHON_ISLAND'] = df['District_VASHON ISLAND']\n",
    "df.drop(['District_FEDERAL WAY','District_LAKE WASHINGTON','District_MERCER ISLAND',\n",
    "                         'District_SNOQUALMIE VALLEY','District_VASHON ISLAND'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = ['District_BELLEVUE','District_ENUMCLAW','District_FEDERAL_WAY','District_HIGHLINE',\n",
    "                         'District_ISSAQUAH','District_KENT','District_LAKE_WASHINGTON','District_MERCER_ISLAND',\n",
    "                         'District_NORTHSHORE','District_RENTON','District_RIVERVIEW','District_SEATTLE',\n",
    "                         'District_SHORELINE','District_SKYKOMISH','District_SNOQUALMIE_VALLEY','District_TAHOMA',\n",
    "                         'District_TUKWILA','District_VASHON_ISLAND']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An already cleaned kc_house_data.csv was used to extract school data and map using ARCGIS rendering the following formatting obsolete, however it will still be listed for clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Column formatting & dealing with missing values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Grade to Numeric\n",
    "# df['grade'] = df['grade'].str.slice(0,2).str.strip()\n",
    "# df['grade'] = df['grade'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting View to Numeric\n",
    "# df['view'].fillna('NONE', inplace=True)\n",
    "# df['view'].replace('NONE', '0', inplace=True)\n",
    "# df['view'].replace('FAIR', '1', inplace=True)\n",
    "# df['view'].replace('AVERAGE', '2', inplace=True)\n",
    "# df['view'].replace('GOOD', '3', inplace=True)\n",
    "# df['view'].replace('EXCELLENT', '4', inplace=True)\n",
    "# df['view'] = df['view'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dropping outliers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping outliers\n",
    "# price_mean = np.mean(df['price'])\n",
    "# cut_off = np.std(df['price']) * 3\n",
    "# df.drop(df[df['price'] > price_mean + cut_off].index, inplace = True)\n",
    "# df.drop(df[df['price'] < price_mean - cut_off].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Presentation Charting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "sns.histplot(df['price'], ax=ax, bins=50, kde=True, linewidth=2)\n",
    "plt.plot([df['mean_price'], df['mean_price']], [0, 1500], linewidth=5, color='red')\n",
    "plt.title('Average Price Distribution with Mean Line', fontsize=30, fontweight='bold')\n",
    "plt.xlabel('Sale Price in Millions', fontsize=25, fontweight='bold')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('Amount Sold', fontsize=25, fontweight='bold')\n",
    "plt.yticks(fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "df_corrs = df.corr().abs()['price'].sort_values(ascending=False)\n",
    "df_corrs = pd.Series(df_corrs[['grade', 'sqft_living', 'view', 'yr_built']])\n",
    "ax.bar(df_corrs.index, df_corrs.values)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.title('Percent Correlation with Price', fontsize=30, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=25, fontweight='bold')\n",
    "plt.xticks( fontsize=20)\n",
    "plt.ylabel('Percent Correlation', fontsize=25, fontweight='bold')\n",
    "plt.yticks(fontsize=20)\n",
    "ax.set_yticklabels(['0%', '20%', '40%', '60%', '80%', '100%']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_no_district = df[['price', 'sqft_living', 'view', 'grade', 'yr_built']]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16,12))\n",
    "\n",
    "columns = df_no_district.columns\n",
    "\n",
    "for col, ax in zip(columns, axes.flatten()):\n",
    "    ax.hist(df_no_district[col].dropna(), bins='auto')\n",
    "    ax.set_title(col)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created 6 models. Baseline model, First model, Good model, Great model, Awesome model, and Amazing model.\n",
    "In the end, we decided to go with Great model as our Final model. \n",
    "\n",
    "Our Final model, known as \"Great Model\" has the follow predictors:\n",
    "1. Square feet living\n",
    "2. School Districts\n",
    "3. Building Grade\n",
    "\n",
    "We decided Great Model is adequate with an R-squared value of .72 with only 3 predictors. The simplicity of it makes it easy to understand and use while also explaining such a high amount of the variance in price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr = df.iloc[:, 2:21].corr()\n",
    "# Set up figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 15))\n",
    "# Plot a heatmap of the correlation matrix, with both\n",
    "# numbers and colors indicating the correlations\n",
    "sns.heatmap(\n",
    "    # Specifies the data to be plotted\n",
    "    data=corr,\n",
    "    # The mask means we only show half the values,\n",
    "    # instead of showing duplicates. It's optional.\n",
    "    mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "    # Specifies that we should use the existing axes\n",
    "    ax=ax,\n",
    "    # Specifies that we want labels, not just colors\n",
    "    annot=True,\n",
    "    # Customizes colorbar appearance\n",
    "    cbar_kws={\"label\": \"Correlation\", \"orientation\": \"horizontal\", \"pad\": .1, \"extend\": \"both\"},\n",
    "    annot_kws={\"fontsize\":10}\n",
    ")\n",
    "# Customize the plot appearance\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes (Including Target)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Helper Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearity_plot(x, y):\n",
    "    x = pd.DataFrame(x)\n",
    "    y = pd.DataFrame(y)\n",
    "    cols = 1\n",
    "    if x.shape[1] % 2 == 0:\n",
    "        cols = 2\n",
    "    elif x.shape[1] < 2:\n",
    "        cols = 1\n",
    "    else:\n",
    "        cols = 3\n",
    "    rows = 1\n",
    "    if x.shape[1] // cols == 0:\n",
    "        rows = 1\n",
    "    elif x.shape[1] % cols > 0:\n",
    "        rows = x.shape[1] // cols + 1\n",
    "    else:\n",
    "        rows = rows = x.shape[1] // cols \n",
    "    fig, axes = plt.subplots(ncols=cols, nrows=rows, figsize=(16,10), squeeze=False)\n",
    "    fig.set_tight_layout(True)\n",
    "    for index, col in enumerate(x.columns):\n",
    "        ax = axes[index//cols][index%cols]\n",
    "        ax.scatter(x[col], y, alpha=0.2)\n",
    "        ax.set_xlabel(col, fontsize=20)\n",
    "        ax.set_ylabel(\"Listing price\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr_randomly(data, sample_pt=None, ntimes=500):\n",
    "    '''\n",
    "    Takes in features & targets from `data` to train a linear regression with a\n",
    "    random sample `ntimes`. It then returns a list of R2 scores, RMSEs, and the \n",
    "    predictions from a provided data point of features `sample_pt`.\n",
    "    '''\n",
    "    # To save all of our predictions\n",
    "    r2 = []\n",
    "    rmse = []\n",
    "    # Only return predictions if there is something to predict (sample_pt given)\n",
    "    point_preds = [] if (sample_pt is not None) else None\n",
    "\n",
    "    # We'll repeat this little experiment to see how the model does\n",
    "    for i in range(ntimes):\n",
    "        # Creating a random sample of data to train on\n",
    "        df_sample = data.sample(5000, replace=True)\n",
    "        y = df_sample.price\n",
    "        X = df_sample.drop('price', axis=1)\n",
    "\n",
    "        # Our linear regression model about to be trained\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X, y)\n",
    "\n",
    "        # Making predictions & evaluating on the data we used to train the model\n",
    "        y_hat = lr.predict(X)\n",
    "        rmse.append(np.sqrt(mean_squared_error(y, y_hat)))\n",
    "        r2.append(lr.score(X, y))\n",
    "\n",
    "        # Making a prediction on the one point the model definitely never saw\n",
    "        if sample_pt is not None:\n",
    "            y_hat_pt = lr.predict(sample_pt)\n",
    "            # Getting just the single point to add into list\n",
    "            point_preds.append(y_hat_pt[0])\n",
    "    \n",
    "    return r2, rmse, point_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_box(formula = df.columns[1:6]):\n",
    "    sum_df = df[formula]\n",
    "    sum_sample = sum_df.sample(1)\n",
    "    sum_sample_price = sum_sample.iloc[0]['price']\n",
    "    sum_sample_pt = sum_sample.drop('price', axis=1)\n",
    "    print(sum_sample_price)\n",
    "    # Show my random sample off\n",
    "    print(f'Price of sample: ${sum_sample_price}')    \n",
    "    \n",
    "    # Run 100 linear regression trainings on some random data from df and compare\n",
    "    # it with the random sample point\n",
    "    r2_sum, rmse_sum, pt_preds_sum = train_lr_randomly(\n",
    "                                                            data=sum_df, \n",
    "                                                            sample_pt=sum_sample_pt,\n",
    "                                                            ntimes=500                            \n",
    "    )\n",
    "    \n",
    "    ax = sns.boxplot(x=pt_preds_sum);\n",
    "    ax = sns.swarmplot(x=pt_preds_sum, color='orange', ax=ax)\n",
    "    ax.set_title(f'Predicting Sample Pt Price: ${sum_sample_price:,.2f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model takes the mean price of the whole dataset. It is a horizontal line which sits at the mean price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_y = df['price']\n",
    "baseline_X = df.drop(['price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_X_train, baseline_X_test, baseline_y_train, baseline_y_test = train_test_split(baseline_X, baseline_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "baseline_X_train_scaled = scaler.fit_transform(baseline_X_train)\n",
    "baseline_X_test_scaled = scaler.fit_transform(baseline_X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Train Mean: {baseline_y_train.mean()}, Test Mean: {baseline_y_test.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "dummy_regr.fit(baseline_X_train_scaled, baseline_y_train)\n",
    "dummy_regr.predict(baseline_X_train_scaled)\n",
    "dummy_regr.score(baseline_X_test_scaled, baseline_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_X_train_scaled = pd.DataFrame(baseline_X_train_scaled)\n",
    "baseline_X_train_scaled.columns = baseline_X.columns\n",
    "baseline_X_test_scaled = pd.DataFrame(baseline_X_test_scaled)\n",
    "baseline_X_test_scaled.columns = baseline_X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Interpreting the baseline model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model definitely follows the linearity assumption. The distribution of errors is slightly skewed right. Many of the VIF values are above 10 and this indicates that there is high multicollinearity. It passes the homoscedasticity test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Assumptions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEARITY\n",
    "df['mean_price']=df['price'].mean()\n",
    "linearity_plot(df['price'], df['mean_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='sqft_living', y='price', data=df, line_kws={'color': 'black'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NORMALITY\n",
    "# Normal distribution of errors check\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(baseline_X_train_scaled, baseline_y_train)\n",
    "baseline_model.score(baseline_X_test_scaled, baseline_y_test)\n",
    "preds = baseline_model.predict(baseline_X_test_scaled)\n",
    "residuals = (baseline_y_test - preds)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='r', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking MULTICOLLINEARITY \n",
    "# (VIF NEEDS TO BE <5 ALL CATEGORIES)\n",
    "vif = [variance_inflation_factor(baseline_X_train_scaled.values, i) for i in range(baseline_X_train_scaled.shape[1])]\n",
    "pd.Series(vif, index=baseline_X_train_scaled.columns, name=\"Variance Inflation Factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#HOMOSCEDASTICITY\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(preds, residuals, alpha=0.5)\n",
    "ax.plot(preds, [0 for i in range(len(baseline_X_test_scaled))], color='black')\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Mass Tests*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dropping Unnecessary Columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id','bedrooms','bathrooms','sqft_lot','floors','waterfront','condition','sqft_above','sqft_basement', 'yr_renovated',\n",
    "                         'zipcode','lat','long','sqft_living15','sqft_lot15','renovated_less_10yrs',\n",
    "                         'has_basement','Distance_to_School_ft','mean_price'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model: Square Feet Living & Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model includes Square Feet Living plotted against Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_y = df[\"price\"]\n",
    "first_formula = ['sqft_living']\n",
    "first_full_formula = ['price'] + first_formula\n",
    "first_X = df[first_formula]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_X_train, first_X_test, first_y_train, first_y_test = train_test_split(first_X, first_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "first_X_train_scaled = scaler.fit_transform(first_X_train)\n",
    "first_X_test_scaled = scaler.fit_transform(first_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = LinearRegression()\n",
    "\n",
    "# Fit the model on X_train_final and y_train\n",
    "first_model.fit(first_X_train_scaled, first_y_train)\n",
    "\n",
    "# Score the model on X_test_final and y_test\n",
    "# (use the built-in .score method)\n",
    "first_model.score(first_X_test_scaled, first_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Train RMSE: {mean_squared_error(first_y_train, first_model.predict(first_X_train_scaled), squared=False)}')\n",
    "print(f'Test RMSE: {mean_squared_error(first_y_test, first_model.predict(first_X_test_scaled), squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(first_y_train, sm.add_constant(first_X_train)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_X_train_scaled = pd.DataFrame(first_X_train_scaled)\n",
    "first_X_train_scaled.columns = first_X.columns\n",
    "first_X_test_scaled = pd.DataFrame(first_X_test_scaled)\n",
    "first_X_test_scaled.columns = first_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(first_X_train[first_formula], first_y_train, alpha=0.5, color ='white', edgecolor='blue')\n",
    "ax.set_xlabel('Square Feet Living')\n",
    "ax.set_ylabel(\"Price (1M's)\")\n",
    "ax.set_title('Square Feet to Price')\n",
    "plt.plot(np.unique(first_X_train['sqft_living']), np.poly1d(np.polyfit(first_X_train['sqft_living'], first_y_train, 1))\n",
    "         (np.unique(first_X_train['sqft_living'])),linewidth=3.0,color='black');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Interpreting the first model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Square Feet Living drives up Price. It has an R-squared of .445. It explains 44.5% of the variance in price by itself. It is the most correlated variable and individually most significant in our findings. Our first model definitely follows the linearity assumption. The distribution of errors is very slightly heavy on the tails. Multicollinearity is not applicable for one predictor. It passes the homoscedasticity test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Assumptions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#LINEARITY\n",
    "linearity_plot(first_X_train, first_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NORMALITY\n",
    "import scipy.stats as stats\n",
    "preds_1st = first_model.predict(first_X_test_scaled)\n",
    "residuals = (first_y_test - preds_1st)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='r', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#HOMOSCEDASTICITY\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(preds_1st, residuals, alpha=0.5)\n",
    "ax.plot(preds, [0 for i in range(len(first_X_test_scaled))], color='black')\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_box(first_full_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Model: Square Feet Living and School District"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the good model, we have 2 predictors. Those predictors are square feet living and school district. Our school district variable consists of the 18 districts located in King County. Those districts are Bellevue, Enumclaw, Federal Way, Highline, Issaquah, Kent, Lake Washington, Mercer Island, Northshore, Renton, Riverview, Seattle, Shoreline, Skykomish, Snoqualmie Valley, Tahoma, Tukwila, Vashon Island."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_y = df[\"price\"]\n",
    "good_formula = first_formula + ['District_BELLEVUE','District_ENUMCLAW','District_FEDERAL_WAY',\n",
    "                 'District_HIGHLINE','District_ISSAQUAH','District_KENT','District_LAKE_WASHINGTON','District_MERCER_ISLAND',\n",
    "                 'District_NORTHSHORE','District_RENTON','District_RIVERVIEW','District_SEATTLE','District_SHORELINE',\n",
    "                 'District_SKYKOMISH','District_SNOQUALMIE_VALLEY','District_TAHOMA','District_TUKWILA',\n",
    "                 'District_VASHON_ISLAND']\n",
    "good_full_formula = ['price'] + good_formula\n",
    "good_X = df[good_formula]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_X_train, good_X_test, good_y_train, good_y_test = train_test_split(good_X, good_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "good_X_train_scaled = scaler.fit_transform(good_X_train)\n",
    "good_X_test_scaled = scaler.fit_transform(good_X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_model = LinearRegression()\n",
    "\n",
    "# Fit the model on X_train_final and y_train\n",
    "good_model.fit(good_X_train_scaled, good_y_train)\n",
    "\n",
    "# Score the model on X_test_final and y_test\n",
    "# (use the built-in .score method)\n",
    "good_model.score(good_X_test_scaled, good_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Train RMSE: {mean_squared_error(good_y_train, good_model.predict(good_X_train_scaled), squared=False)}')\n",
    "print(f'Test RMSE: {mean_squared_error(good_y_test, good_model.predict(good_X_test_scaled), squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sm.OLS(good_y_train, sm.add_constant(good_X_train)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_X_train_scaled = pd.DataFrame(good_X_train_scaled)\n",
    "good_X_train_scaled.columns = good_X.columns\n",
    "good_X_test_scaled = pd.DataFrame(good_X_test_scaled)\n",
    "good_X_test_scaled.columns = good_X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Interpreting the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good model has an R-squared of .683. Square feet living combined with District explains 68.3% of the variance in price. The inclusion of District raised our R-squared from .44 to .68, for an increase of .24. Our good model definitely follows the linearity assumption. Square feet living follows linearity, and district is linear by definition as a categorical variable. The distribution of errors is skewed right. Good model has low multicollinearity in general, with all VIF below 5 except for District_SEATTLE at 7. It passes the homoscedasticity test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Assumptions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "linearity_plot(good_X_train['sqft_living'], good_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#NORMALITY\n",
    "good_X_test_scaled = pd.DataFrame(good_X_test_scaled)\n",
    "preds_good = good_model.predict(good_X_test_scaled)\n",
    "residuals = (good_y_test - preds_good)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='r', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking MULTICOLLINEARITY \n",
    "# (VIF NEEDS TO BE <5 ALL CATEGORIES)\n",
    "vif = [variance_inflation_factor(good_X_train_scaled.values, i) for i in range(good_X_train_scaled.shape[1])]\n",
    "pd.Series(vif, index=good_X_train_scaled.columns, name=\"Variance Inflation Factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#HOMOSCEDASTICITY\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(preds_good, residuals, alpha=0.5)\n",
    "ax.plot(preds_good, [0 for i in range(len(good_X_test_scaled))], color='black')\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOMOSCEDASTICITY BORDERLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_box(good_full_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great Model: Square feet living, School districts, and Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Great model uses square feet living, school districts, and grade as our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_y = df[\"price\"]\n",
    "great_formula = good_formula + ['grade']\n",
    "great_full_formula = ['price'] + great_formula\n",
    "great_X = df[great_formula]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_X_train, great_X_test, great_y_train, great_y_test = train_test_split(great_X, great_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "great_X_train_scaled = scaler.fit_transform(great_X_train)\n",
    "great_X_test_scaled = scaler.fit_transform(great_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_model = LinearRegression()\n",
    "\n",
    "# Fit the model on X_train_final and y_train\n",
    "great_model.fit(great_X_train_scaled, great_y_train)\n",
    "\n",
    "# Score the model on X_test_final and y_test\n",
    "# (use the built-in .score method)\n",
    "great_model.score(great_X_test_scaled, great_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train RMSE: {mean_squared_error(great_y_train, great_model.predict(great_X_train_scaled), squared=False)}')\n",
    "print(f'Test RMSE: {mean_squared_error(great_y_test, great_model.predict(great_X_test_scaled), squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sm.OLS(great_y_train, sm.add_constant(great_X_train)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great_X_train_scaled = pd.DataFrame(great_X_train_scaled)\n",
    "great_X_train_scaled.columns = great_X.columns\n",
    "great_X_test_scaled = pd.DataFrame(great_X_test_scaled)\n",
    "great_X_test_scaled.columns = great_X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Interpreting the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great model has an R-squared of .723. Square feet living, District, and Grade explains 72.3% of the variance in price. This is our first model which crosses the .7 R-squared threshold. The inclusion of Grade raised our R-squared from .68 to .72, for an increase of .04. Our great model definitely follows the linearity assumption, given that Districts are linear by definition, and Grade is also linear. The distribution of errors is unfortunately skewed right. Great model has low multicollinearity in general, with all VIF below 5 except for District_SEATTLE at 7. It passes the homoscedasticity test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Assumptions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Linearity\n",
    "linearity_plot(great_X_train[['sqft_living', 'grade']], great_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NORMALITY\n",
    "great_X_test_scaled = pd.DataFrame(great_X_test_scaled)\n",
    "preds_great = great_model.predict(great_X_test_scaled)\n",
    "residuals = (great_y_test - preds_great)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='r', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking MULTICOLLINEARITY \n",
    "# (VIF NEEDS TO BE <5 ALL CATEGORIES)\n",
    "vif = [variance_inflation_factor(great_X_train_scaled.values, i) for i in range(great_X_train_scaled.shape[1])]\n",
    "pd.Series(vif, index=great_X_train_scaled.columns, name=\"Variance Inflation Factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#HOMOSCEDASTICITY\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(preds_great, residuals, alpha=0.5)\n",
    "ax.plot(preds_great, [0 for i in range(len(great_X_test_scaled))], color='black')\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_box(great_full_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awesome Model: sqft_living, school districts, grade, year built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome model includes square feet living, school districts, grade, and year built as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awesome_y = df[\"price\"]\n",
    "awesome_formula = great_formula + ['yr_built']\n",
    "awesome_full_formula = ['price'] + awesome_formula\n",
    "awesome_X = df[awesome_formula]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awesome_X_train, awesome_X_test, awesome_y_train, awesome_y_test = train_test_split(awesome_X, awesome_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "awesome_X_train_scaled = scaler.fit_transform(awesome_X_train)\n",
    "awesome_X_test_scaled = scaler.fit_transform(awesome_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awesome_model = LinearRegression()\n",
    "\n",
    "# Fit the model on X_train_final and y_train\n",
    "awesome_model.fit(awesome_X_train_scaled, awesome_y_train)\n",
    "\n",
    "# Score the model on X_test_final and y_test\n",
    "# (use the built-in .score method)\n",
    "awesome_model.score(awesome_X_test_scaled, awesome_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train RMSE: {mean_squared_error(awesome_y_train, awesome_model.predict(awesome_X_train_scaled), squared=False)}')\n",
    "print(f'Test RMSE: {mean_squared_error(awesome_y_test, awesome_model.predict(awesome_X_test_scaled), squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sm.OLS(awesome_y_train, sm.add_constant(awesome_X_train)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awesome_X_train_scaled = pd.DataFrame(awesome_X_train_scaled)\n",
    "awesome_X_train_scaled.columns = awesome_X.columns\n",
    "awesome_X_test_scaled = pd.DataFrame(awesome_X_test_scaled)\n",
    "awesome_X_test_scaled.columns = awesome_X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Interpreting the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome model has an R-squared of .742. Building on Great model, adding yr_built explains 74.2% of the variance in price. The inclusion of yr_built raised our R-squared from .72 to .74, for an increase of .02. Our awesome model mostly follow the linearity assumption, but the linearity of yr_built is vague. The distribution of errors is skewed right. Good model has low multicollinearity in general, with all VIF below 5 except for District_SEATTLE at 7. It passes the homoscedasticity test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Assumptions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Linearity\n",
    "linearity_plot(awesome_X_train[['sqft_living', 'grade', 'yr_built']], awesome_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NORMALITY\n",
    "awesome_X_test_scaled = pd.DataFrame(awesome_X_test_scaled)\n",
    "preds_awesome = awesome_model.predict(awesome_X_test_scaled)\n",
    "residuals = (awesome_y_test - preds_awesome)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='r', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking MULTICOLLINEARITY \n",
    "# (VIF NEEDS TO BE <5 ALL CATEGORIES)\n",
    "vif = [variance_inflation_factor(awesome_X_train_scaled.values, i) for i in range(awesome_X_train_scaled.shape[1])]\n",
    "pd.Series(vif, index=awesome_X_train_scaled.columns, name=\"Variance Inflation Factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOMOSCEDASTICITY\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(preds_awesome, residuals, alpha=0.5)\n",
    "ax.plot(preds_awesome, [0 for i in range(len(awesome_X_test_scaled))], color='black')\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_box(awesome_full_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazing Model: sqft_living, school districts, grade, year built, view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing model includes square feet living, school districts, grade, year built, and view as predictors.\n",
    "This makes for a total of 5 predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_y = df[\"price\"]\n",
    "amazing_formula = awesome_formula + ['view']\n",
    "amazing_full_formula = ['price'] + amazing_formula\n",
    "amazing_X = df[amazing_formula]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_X_train, amazing_X_test, amazing_y_train, amazing_y_test = train_test_split(amazing_X, amazing_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "amazing_X_train_scaled = scaler.fit_transform(amazing_X_train)\n",
    "amazing_X_test_scaled = scaler.fit_transform(amazing_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_model = LinearRegression()\n",
    "\n",
    "# Fit the model on X_train_final and y_train\n",
    "amazing_model.fit(amazing_X_train_scaled, amazing_y_train)\n",
    "\n",
    "# Score the model on X_test_final and y_test\n",
    "# (use the built-in .score method)\n",
    "amazing_model.score(amazing_X_test_scaled, amazing_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train RMSE: {mean_squared_error(amazing_y_train, amazing_model.predict(amazing_X_train_scaled), squared=False)}')\n",
    "print(f'Test RMSE: {mean_squared_error(amazing_y_test, amazing_model.predict(amazing_X_test_scaled), squared=False)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sm.OLS(amazing_y_train, sm.add_constant(amazing_X_train)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazing_X_train_scaled = pd.DataFrame(amazing_X_train_scaled)\n",
    "amazing_X_train_scaled.columns = amazing_X.columns\n",
    "amazing_X_test_scaled = pd.DataFrame(amazing_X_test_scaled)\n",
    "amazing_X_test_scaled.columns = amazing_X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Touching on School Districts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since school districts is a more complex variable, we wanted to take some time to explain it. It was originally one column with 18 categores. We created dummy variables in order to model it. The result of that is 18 columns. Here, we have modeled all the districts by themselves in an OLS. We have taken data from the amazing train set for this OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = amazing_X_train.copy().iloc[:,1:-3]\n",
    "temp_df\n",
    "sm.OLS(amazing_y_train, sm.add_constant(temp_df)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Interpreting the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing model has an R-squared of .756. This is the highest R-squared value we were able to reach. \n",
    "This was built by using square feet living, school district, grade, year built, and view.\n",
    "The inclusion of view raised our R-squared from .742 to .756, for an increase of .14.\n",
    "Our amazing model has a couple predictors that are sketchy with the linearity assumption. Those variables are view and yr_built. This is part of the reason we decided not to use amazing model or awesome model as our final model. The distribution of errors is skewed right. Amazing model has low multicollinearity in general, with all VIF below 5 except for District_SEATTLE at 7. It passes the homoscedasticity test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Linearity\n",
    "linearity_plot(amazing_X_train[['sqft_living', 'grade', 'yr_built', 'view']], amazing_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NORMALITY\n",
    "amazing_X_test_scaled = pd.DataFrame(amazing_X_test_scaled)\n",
    "preds_amazing = amazing_model.predict(amazing_X_test_scaled)\n",
    "residuals = (amazing_y_test - preds_amazing)\n",
    "sm.graphics.qqplot(residuals, dist=stats.norm, line='r', fit=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Checking MULTICOLLINEARITY \n",
    "# (VIF NEEDS TO BE <5 ALL CATEGORIES)\n",
    "vif = [variance_inflation_factor(amazing_X_train_scaled.values, i) for i in range(amazing_X_train_scaled.shape[1])]\n",
    "pd.Series(vif, index=amazing_X_train_scaled.columns, name=\"Variance Inflation Factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#HOMOSCEDASTICITY\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(preds_amazing, residuals, alpha=0.5)\n",
    "ax.plot(preds_amazing, [0 for i in range(len(amazing_X_test_scaled))], color='black')\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Actual - Predicted Value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_box(amazing_full_formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Results - Great Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use Great Model as our Final model. This model did very well with 3 predictors. Those predictors are square feet living, school district, and grade. The reasoning for selecting Great Model even though some of the following models had higher R-squared values was because it was simple, yet understandable. The following models seemed to muddy the waters when it came to the linearity assumption. \n",
    "\n",
    "The Great Model is able to explain 72.3% of the variance in price of King County Home Sales in 2014-15 with just 3 predictors.\n",
    "This is a good R-squared value as it is above the 70% threshold. The model had a Train RMSE of 137070.49516065692 and a Test RMSE of 136447.31048874996. These are very close to each other, meaning the fit of the model is not underfit or overfit.\n",
    "\n",
    "The Great Model does a great job at helping our stakeholder, the real estate agency, understand the gist of what drives home prices in the King County area. More specifically, it gives invaluable information about which school district they should target for either top end or bottom end location values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.OLS(great_y_train, sm.add_constant(great_X_train)).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train RMSE: {mean_squared_error(great_y_train, great_model.predict(great_X_train_scaled), squared=False)}')\n",
    "print(f'Test RMSE: {mean_squared_error(great_y_test, great_model.predict(great_X_test_scaled), squared=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model showed us that the most important predictors to look at in a home are going to be square feet living, school district, and building grade. To relate this back to our stakeholder, we want to inform the real estate agency to focus on these elements in their deals. We can also help them by allowing them to plot different values in our model and giving them a rough valuation based on these variables.\n",
    "\n",
    "Our model separates each home into 3 basic elements that contribute to value:\n",
    "1. Home size & usable space (sqft_living)\n",
    "2. Location (school district)\n",
    "3. Upgrades, amenities, & design elements (grade)\n",
    "\n",
    "To improve our model in the future, we could bin house sales by neighborhood. We could then compare home sales within individual neighborhood and model what features accounted for the discrepancies in sales price. This would give us a more detailed look at what features distinguish home valuations. This would also help the stakeholder with their offers and listings in a more detailed and refined level. More data would also be very useful. Another thing we could improve on is using more location metrics. The location metrics we would want to add include proximity to shopping, amenities, entertainment, recreation, airports, parks, schools, landmarks, tourist attractions, and employment opportunities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
